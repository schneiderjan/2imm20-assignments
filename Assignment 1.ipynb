{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<style>html, body{overflow-y: visible !important} .CodeMirror{min-width:105% !important;} .rise-enabled .CodeMirror, .rise-enabled .output_subarea{font-size:140%; line-height:1.2; overflow: visible;} .output_subarea pre{width:110%}</style>''') # For slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Foundations of Data Mining: Assignment 1\n",
    "\n",
    "Please complete all assignments in this notebook. You should submit this notebook, as well as a PDF version (See File > Download as)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please fill in your names here\n",
    "NAME_STUDENT_1 = \"Jan-Niklas Schneider 1260421\"\n",
    "NAME_STUDENT_2 = \"Georgiana Manolache 0876359\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from preamble import *\n",
    "plt.rcParams['savefig.dpi'] = 100 # This controls the size of your figures\n",
    "# Comment out and restart notebook if you only want the last output of each cell.\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MoneyBall (5 points, 1+2+1+1)\n",
    "In the early 2000s, 2 baseball scouts completely changed the game of baseball by analysing the available data about baseball players and hiring the best ones.\n",
    "The [MoneyBall dataset](https://www.openml.org/d/41021) contains this data (click the link for more details). The goal is to accurately predict the number of 'runs' each player can score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "moneyball = oml.datasets.get_dataset(41021) # Download MoneyBall data\n",
    "# Get the predictors X and the target y\n",
    "X, y, attribute_names = moneyball.get_data(target=moneyball.default_target_attribute, return_attribute_names=True)\n",
    "# Describe the data with pandas, just to get an overview\n",
    "ballframe = pd.DataFrame(X, columns=attribute_names)\n",
    "ballframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 . Visually explore the data. Plot the distribution of each feature (e.g. histograms), as well as the target. Visualize the dependency of the target on each feature (use a 2d scatter plot). Is there anything that stands out? Is there something that you think might require special treatment?\n",
    "- Feel free to create additional plots that help you understand the data\n",
    "- Only visualize the data, you don't need to change it (yet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just have a look at some details of the data frame before actually visualize sutff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(attribute_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ballframe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ballframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 15):\n",
    "    print(ballframe.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some NaN's are present in RankSeason and RankPlayoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('targets')\n",
    "print(y)\n",
    "print(\"nr of targets {0}\".format(len(np.unique(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ballframe['Team'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some teams are more often in data; probably due to having more games in play offs.\n",
    "All data is encoded as floats.\n",
    "What I want to do now is to see whats different between those teams with 47 appearances to the other ones.\n",
    "[An Examination of the Moneyball Theory: A Baseball Statistical Analysis](http://thesportjournal.org/article/an-examination-of-the-moneyball-theory-a-baseball-statistical-analysis/)\n",
    "this link might be interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p1 = scatter_matrix(ballframe, alpha=0.2, figsize=(20, 16), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have some features that are correlated and seem to draw decent linear function in a 2D environment. These features are SLG - OBP, BA - OBP, RA - OOBP, RA - OSLG, OOBP - OSLG. Lets do some additional plots for those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['SLG', 'OBP', 'BA', 'RA', 'OOBP', 'OSLG']\n",
    "p2 = scatter_matrix(ballframe[columns], alpha=0.2, figsize=(10, 10), diagonal='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 . Compare all linear regression algorithms that we covered in class (Linear Regression, Ridge, Lasso and ElasticNet), as well as kNN. Evaluate using cross-validation and the $R^2$ score, with the default parameters. Does scaling the data with StandardScaler help? Provide a concise but meaningful interpretation of the results.\n",
    "- Preprocess the data as needed (e.g. are there nominal features that are not ordinal?). If you don't know how to proceed, remove the feature and continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_cols = ['RankSeason', 'RankPlayoffs']\n",
    "ballframe = ballframe.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply filled the missing values for RankSeason and RankPlayoffs by 0 since they seem to be indepent of other variables as observed in the scatter matrices above. Additionally, the runs (target) should not be affected by the season or playoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(ballframe, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train: {0} \".format(X_train.shape))\n",
    "print(\"X_test: {0} \".format(X_test.shape))\n",
    "print(\"y_train: {0} \".format(y_train.shape))\n",
    "print(\"y_test: {0} \".format(y_test.shape))\n",
    "print(\"For scaled data the shape is the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the assignment it said to also consider also a StandardScaler. I will make use of if for the logistic regression. I am aware that scaling the input will have no noticable effect on the regression. Eventually accuracy will slightly differ because we do a second train/test-split on the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# initialize all regressors and knn\n",
    "lr = LinearRegression()\n",
    "rid = Ridge()\n",
    "lasso = Lasso()\n",
    "elastic = ElasticNet()\n",
    "knn = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "print(std_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(StandardScaler().fit_transform(ballframe), y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# regular data\n",
    "lr_scores = cross_val_score(lr, X_train, y_train, cv=5)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print(\"Logistic regression test scores:\")\n",
    "print(\"cross val scores: {}\".format(lr_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_lr)))\n",
    "print(\"Average cv score: {:.2f}\".format(lr_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(lr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled data\n",
    "lr_scores = cross_val_score(lr, X_train_scaled, y_train_scaled, cv=5)\n",
    "lr.fit(X_train_scaled, y_train_scaled)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic regression test scores with scaled value:\")\n",
    "print(\"cross val scores: {}\".format(lr_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_lr)))\n",
    "print(\"Average cv score: {:.2f}\".format(lr_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(lr_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scores = cross_val_score(rid, X_train, y_train, cv=5)\n",
    "rid.fit(X_train, y_train)\n",
    "y_pred_ridge = rid.predict(X_test)\n",
    "\n",
    "print(\"Ridge test scores:\")\n",
    "print(\"CV scores: {}\".format(ridge_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_ridge)))\n",
    "print(\"Average cv score: {:.2f}\".format(ridge_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(ridge_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_scores = cross_val_score(lasso, X_train, y_train, cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "\n",
    "print(\"Lasso test scores:\")\n",
    "print(\"CV scores: {}\".format(lasso_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_lasso)))\n",
    "print(\"Average cv score: {:.2f}\".format(lasso_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(lasso_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_scores = cross_val_score(elastic, X_train, y_train, cv=5)\n",
    "elastic.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic.predict(X_test)\n",
    "\n",
    "print(\"Elastic test scores:\")\n",
    "print(\"CV scores: {}\".format(elastic_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_elastic)))\n",
    "print(\"Average cv score: {:.2f}\".format(elastic_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(elastic_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_scores = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"KNN test scores:\")\n",
    "print(\"cross val scores: {}\".format(knn_scores))\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_knn)))\n",
    "print(\"Average cv score: {:.2f}\".format(knn_scores.mean()))\n",
    "print(\"Variance cv score: {:.4f}\".format(np.var(knn_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3 . Do a default, shuffled train-test split and optimize the linear models for the degree of regularization ($alpha$) and choice of penalty (L1/L2). For Ridge and  Lasso, plot a curve showing the effect of the training and test set performance ($R^2$) while increasing the degree of regularization for different penalties. For ElasticNet, plot a heatmap $alpha \\times l1\\_ratio \\rightarrow R^2$ using test set performance.\n",
    "Report the optimal performance. Again, provide a concise but meaningful interpretation. What does the regularization do? Can you get better results?\n",
    "- Think about how you get the L1/L2 loss. This is not a hyperparameter in regression.\n",
    "- We've seen how to generate such heatmaps in Lecture 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part will look at the optimization of by tuning regularization and penalty by making use of gridsearch and compare to previous default results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "lasso_pipe = Pipeline([\n",
    "    ('estimator', Lasso())\n",
    "])\n",
    "params_lasso = [{'estimator__alpha': [0.1,0.25,0.5,0.75,1]}]\n",
    "# around 0.01 was doing better so we optimize around this value\n",
    "lasso_grid = GridSearchCV(estimator=lasso_pipe, param_grid=params_lasso, cv=ShuffleSplit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(lasso_grid.best_params_)\n",
    "print(lasso_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_best = lasso_grid.best_estimator_\n",
    "y_pred_lasso_best = lasso_best.predict(X_test)\n",
    "\n",
    "print(\"Lasso test scores before optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_lasso)))\n",
    "\n",
    "print(\"Lasso test scores after optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_lasso_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_pipe = Pipeline([\n",
    "    ('estimator', Ridge())\n",
    "])\n",
    "params_ridge = [{'estimator__alpha': [0.1,0.25,0.5,0.75,1]}]\n",
    "ridge_grid = GridSearchCV(estimator=ridge_pipe, param_grid=params_ridge, cv=ShuffleSplit())\n",
    "ridge_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(ridge_grid.best_params_)\n",
    "print(ridge_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_best = ridge_grid.best_estimator_\n",
    "y_pred_ridge_best = ridge_best.predict(X_test)\n",
    "\n",
    "print(\"Ridge test scores before optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_ridge)))\n",
    "\n",
    "print(\"Ridge test scores after optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_ridge_best)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_pipe = Pipeline([\n",
    "    ('estimator', ElasticNet())\n",
    "])\n",
    "params_elastic = [{'estimator__alpha': [0.1, 0.25,0.5,0.75,1],\n",
    "                   'estimator__l1_ratio':[0,0.1,0.25,0.5,0.75,0.9,1]}]\n",
    "elastic_grid = GridSearchCV(estimator=elastic_pipe, param_grid=params_elastic, cv=ShuffleSplit())\n",
    "elastic_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(elastic_grid.best_params_)\n",
    "print(elastic_grid.best_score_)\n",
    "# l1_ratio tends to be 1, hence, we have a L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_best = elastic_grid.best_estimator_\n",
    "y_pred_elastic_best = elastic_best.predict(X_test)\n",
    "\n",
    "print(\"ElasticNet test scores before optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_elastic)))\n",
    "\n",
    "print(\"ElasticNet test scores after optimization:\")\n",
    "print(\"R^2 score: {}\".format(r2_score(y_test, y_pred_elastic_best)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the learning curves in of Lasso/Ridge to see whether our optimized value actually optimize the model or if we under/overfit. For simplicity we will choose the highest scoring regularization values manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(alpha_val, title, train_sizes_, train_scores_, test_scores_):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "#     if ylim is not None:\n",
    "#         plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_scores_mean = np.mean(train_scores_, axis=1)\n",
    "    train_scores_std = np.std(train_scores_, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores_, axis=1)\n",
    "    test_scores_std = np.std(test_scores_, axis=1)\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=alpha_val, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=alpha_val, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"ShuffleSplit score\")\n",
    "    \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes = np.linspace(.1, 1.0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(Lasso(alpha=0.003), ballframe, y, cv=ShuffleSplit(), train_sizes=train_sizes)\n",
    "plot_learning_curve(0.003, \"Lasso\", train_sizes, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge\n",
    "train_sizes, train_scores, test_scores = learning_curve(Ridge(alpha=0.001), ballframe, y, cv=ShuffleSplit(), train_sizes=train_sizes)\n",
    "plot_learning_curve(0.003, \"Ridge\", train_sizes, train_scores, test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_df = pd.DataFrame(elastic_grid.cv_results_)\n",
    "print(heat_df.head(5))\n",
    "heat_mean_scores = np.array(heat_df['mean_test_score']).reshape(5,7)\n",
    "\n",
    "# plot the mean cross-validation scores\n",
    "mglearn.tools.heatmap(heat_mean_scores, xlabel='L1_ratio', xticklabels=params_elastic[0]['estimator__l1_ratio'],\n",
    "                      ylabel='Alpha', yticklabels=params_elastic[0]['estimator__alpha'], cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4 . Visualize the coefficients of the optimized models. Do they agree on which features are\n",
    "important? Compare the results with the feature importances returned by a RandomForest. Do it agree with the linear models? What would look for when scouting for a baseball player?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_coefs = elastic_grid.best_estimator_.named_steps['estimator'].coef_\n",
    "ridge_coefs = ridge_grid.best_estimator_.named_steps['estimator'].coef_\n",
    "lasso_coefs = lasso_grid.best_estimator_.named_steps['estimator'].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    return (df - df.mean()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_coefs_df = pd.DataFrame(elastic_coefs)\n",
    "elastic_coefs_df.index = ballframe.columns\n",
    "elastic_coefs_df = elastic_coefs_df.sort_values(by=0)\n",
    "\n",
    "print('ElasticNet Coefficients:')\n",
    "print(elastic_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_coefs_df = pd.DataFrame(ridge_coefs)\n",
    "ridge_coefs_df.index = ballframe.columns\n",
    "ridge_coefs_df = ridge_coefs_df.sort_values(by=0)\n",
    "\n",
    "print(ridge_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coefs_df = pd.DataFrame(lasso_coefs)\n",
    "lasso_coefs_df.index = ballframe.columns\n",
    "lasso_coefs_df = lasso_coefs_df.sort_values(by=0)\n",
    "print(lasso_coefs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=elastic_coefs_df[0], y=elastic_coefs_df.index, palette=sns.hls_palette(8, l=.3, s=.8)).set_title(\"ElasticNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=lasso_coefs_df[0], y=lasso_coefs_df.index, palette=sns.hls_palette(8, l=.3, s=.8)).set_title(\"Lasso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=ridge_coefs_df[0], y=ridge_coefs_df.index, palette=sns.hls_palette(8, l=.3, s=.8)).set_title(\"Ridge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots we can clearly see which features are most important to these models. These features are clearly OBP and SLG are dominant in all models while in Ridge BA had a certain influence, too. Lets us have a look at the RandomForest whether it agrees with the regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfc.n_features_ )\n",
    "\n",
    "print(rfc.feature_importances_)\n",
    "print(sorted(zip(map(lambda x: round(x, 3), rfc.feature_importances_), ballframe.columns), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the regression models most important features were most definitely SLG and OBP. If we compare the results of a RandomForestClassifier(RFC) with default parameters, we see that the RFC agrees with the most important feature, SLG. RFC, however, regards BA and RA higher. While RA was not present in the regression models, BA was observed to be a more important feature in Ridge. Another notable point in RFC is having surprisingly high importance of Year and Team which could be due to Teams having more impact in certain years such that RFC regards them more important than their actual value. \n",
    "\n",
    "When Scouting for players SLG is clearly the most significant value followed by OBP. Other values to consider are BA and RA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nepalese character recognition (5 points, 1+2+2)\n",
    "The [Devnagari-Script dataset](https://www.openml.org/d/40923) contains 92,000 images (32x32 pixels) of 46 characters from Devanagari script. Your goal is to learn to recognize the right letter given the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "devnagari = oml.datasets.get_dataset(40923) # Download Devnagari data\n",
    "# Get the predictors X and the labels y\n",
    "X, y = devnagari.get_data(target=devnagari.default_target_attribute); \n",
    "classes = devnagari.retrieve_class_labels(target_name='character') # This one takes a while, skip if not needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "# Take some random examples, reshape to a 32x32 image and plot\n",
    "fig, axes = plt.subplots(1, 5,  figsize=(10, 5))\n",
    "for i in range(5):\n",
    "    n = randint(0,90000)\n",
    "    axes[i].imshow(X[n].reshape(32, 32), cmap=plt.cm.gray_r)\n",
    "    axes[i].set_xlabel(\"Class: %s\" % (classes[y[n]]))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1. Evaluate k-Nearest Neighbors, Logistic Regression and RandomForests with their default settings.\n",
    "    - Take a stratified 10% subsample of the data.\n",
    "    - Use the default train-test split and predictive accuracy. Is predictive accuracy a good scoring measure for this problem?\n",
    "    - Try to build the same models on increasingly large samples of the dataset (e.g. 10%, 20%,...). Plot the training time and the predictive performance for each. Stop when the training time becomes prohibitively large (this will be different for different models). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 . Optimize the value for the number of neighbors $k$ (keep $k$ < 50) and the number of trees (keep $n\\_estimators$ < 100) on the stratified 10% subsample.\n",
    "- Use 10-fold crossvalidation and plot $k$ and $n\\_estimators$ against the predictive accuracy. Which value of $k$, $n\\_estimators$ should you pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3 . For the RandomForest, optimize both $n\\_estimators$ and $max\\_features$ at the same time on the entire dataset.\n",
    "- Use a nested cross-validation and a random search over the possible values, and measure the accuracy. Explore how fine-grained this grid/random search can be, given your computational resources. What is the optimal performance you find?\n",
    "- Hint: choose a nested cross-validation that is feasible. Don't use too many folds in the outer loop.\n",
    "- Repeat the grid search and visualize the results as a plot (heatmap) $n\\_estimators \\times max\\_features \\rightarrow ACC$ with ACC visualized as the color of the data point. Try to make the grid as fine as possible. Interpret the results. Can you explain your observations? What did you learn about tuning RandomForests?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Understanding Ensembles (5 points (3+2))\n",
    "Do a deeper analysis of how RandomForests and Gradient Boosting reduce their prediction error. We'll use the MAGIC telescope dataset (http://www.openml.org/d/1120). When high-energy particles hit the atmosphere, they produce chain reactions of other particles called 'showers', and you need to detect whether these are caused by gamma rays or cosmic rays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "magic_data = oml.datasets.get_dataset(1120) # Download MAGIC Telescope data\n",
    "X, y = magic_data.get_data(target=magic_data.default_target_attribute);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Quick visualization\n",
    "X, y, attribute_names = magic_data.get_data(target=magic_data.default_target_attribute, return_attribute_names=True)\n",
    "magic = pd.DataFrame(X, columns=attribute_names)\n",
    "magic.plot(figsize=(20,10))\n",
    "# Also plot the target: 1 = gamma, 0 = background\n",
    "pd.DataFrame(y).plot(figsize=(20,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1 . Do a bias-variance analysis of both algorithms. For each, vary the number of trees on a log scale from 1 to 1024, and plot the bias error (squared), variance, and total error (in one plot per algorithm). Interpret the results. Which error is highest for small ensembles, and which reduced most by each algorithm as you use a larger ensemble? When are both algorithms under- or overfitting? Provide a detailed explanation of why random forests and gradient boosting behave this way.\n",
    "- See lecture 3 for an example on how to do the bias-variance decomposition\n",
    "- To save time, you can use a 10% stratified subsample in your initial experiments, but show the plots for the full dataset in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2 . A _validation curve_ can help you understand when a model starts under- or overfitting. It plots both training and test set error as you change certain characteristics of your model, e.g. one or more hyperparameters. Build validation curves for gradient boosting, evaluated using AUROC, by varying the number of iterations between 1 and 500. In addition, use at least two values for the learning rate (e.g. 0.1 and 1), and tree depth (e.g. 1 and 4). This will yield at least 4 curves. Interpret the results and provide a clear explanation for the results. When is the model over- or underfitting? Discuss the effect of the different combinations learning rate and tree depth and provide a clear explanation.\n",
    "- While scikit-learn has a `validation_curve` function, we'll use a modified version (below) that provides a lot more detail and can be used to study more than one hyperparameter. You can use a default train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Plots validation curves for every classifier in clfs. \n",
    "# Also indicates the optimal result by a vertical line\n",
    "# Uses 1-AUROC, so lower is better\n",
    "def validation_curve(clfs, X_test, y_test, X_train, y_train):\n",
    "    for n,clf in enumerate(clfs):\n",
    "        test_score = np.empty(len(clf.estimators_))\n",
    "        train_score = np.empty(len(clf.estimators_))\n",
    "\n",
    "        for i, pred in enumerate(clf.staged_decision_function(X_test)):\n",
    "            test_score[i] = 1-roc_auc_score(y_test, pred)\n",
    "\n",
    "        for i, pred in enumerate(clf.staged_decision_function(X_train)):\n",
    "            train_score[i] = 1-roc_auc_score(y_train, pred)\n",
    "\n",
    "        best_iter = np.argmin(test_score)\n",
    "        learn = clf.get_params()['learning_rate']\n",
    "        depth = clf.get_params()['max_depth']\n",
    "        test_line = plt.plot(test_score,\n",
    "                             label='learn=%.1f depth=%i (%.2f)'%(learn,depth,\n",
    "                                                                 test_score[best_iter]))\n",
    "\n",
    "        colour = test_line[-1].get_color()\n",
    "        plt.plot(train_score, '--', color=colour)\n",
    "        \n",
    "        plt.xlabel(\"Number of boosting iterations\")\n",
    "        plt.ylabel(\"1 - area under ROC\")\n",
    "        plt.axvline(x=best_iter, color=colour)\n",
    "        \n",
    "    plt.legend(loc='best')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
